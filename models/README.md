# Machine Learning Models and Scripts

This directory contains the Python scripts used for data processing, feature engineering, and training the machine learning models for the Ocean Intelligence Platform.

## Overview

The workflow is divided into several key scripts:

1.  **`extract_frames.py`**: Preprocesses raw video data by extracting individual frames to be used for training an image classification model.
2.  **`data_fusion.py`**: Merges biological occurrence data with oceanographic data to create a rich dataset for training a species distribution model.
3.  **Model Training (Not Pictured)**: The scripts in `scripts/utils` likely use the data generated by the above scripts to train the final models. The trained models are stored in the `trained_models` directory.

## Scripts

### Frame Extraction

- **Script:** `extract_frames.py`
- **Purpose:** To extract image frames from video files. This is a common preprocessing step for computer vision tasks, where a model is trained to detect or classify objects (like fish) in images.
- **Usage:**
  ```bash
  python models/extract_frames.py
  ```
- **Process:**
  - Reads video files (`.mp4`) from the `data/raw_data/noaa_images` directory.
  - Extracts frames at a set interval.
  - Saves the frames as JPEG images into `models/data/raw_data/images/positive` and `models/data/raw_data/images/negative` directories based on the source video.

### Data Fusion

- **Script:** `data_fusion.py`
- **Purpose:** To combine two different types of datasets:
  1.  **Species Occurrence Data:** Information on where a particular species has been observed (from a Darwin Core Archive).
  2.  **Oceanographic Data:** Environmental data such as water temperature and salinity (from WOD files).
- **Usage:**
  ```bash
  # To run the full fusion process
  python models/data_fusion.py

  # To process and clean only the occurrence data
  python models/data_fusion.py --occ-only
  ```
- **Process:**
  - Loads and cleans the species occurrence data.
  - Loads and processes the oceanographic data.
  - Merges the two datasets by aligning them on their geographical coordinates (latitude/longitude) and timestamps.
  - Saves the final fused dataset to `data/processed_data` as both a CSV and a Parquet file. This fused dataset can then be used to train a model that predicts species distribution based on environmental factors.

## Trained Models

- **Directory:** `trained_models/`
- **Purpose:** This directory stores the output of the model training process.
- **Contents:**
  - `species_rf.pkl`: A pickled file containing the trained Random Forest model for species prediction.
  - `species_rf_metrics.json`: A JSON file containing performance metrics for the trained model (e.g., accuracy, precision, recall).
  - `confusion_matrix.png` & `species_distribution.png`: Visualizations of the model's performance and predictions.

## Dependencies

The required Python packages for these scripts are listed in `models/requirements.txt`.

**Installation:**
```bash
pip install -r models/requirements.txt
```
